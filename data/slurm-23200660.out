Loading Llama 2...
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
/pfs/proj/nobackup/fs/projnb10/hpc2n2023-115/env/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
Loaded in 21.41 seconds
Traceback (most recent call last):
  File "/proj/nobackup/hpc2n2023-115/jailbreaker/chat.py", line 52, in <module>
    fire.Fire(main)
  File "/pfs/proj/nobackup/fs/projnb10/hpc2n2023-115/env/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/pfs/proj/nobackup/fs/projnb10/hpc2n2023-115/env/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/pfs/proj/nobackup/fs/projnb10/hpc2n2023-115/env/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/proj/nobackup/hpc2n2023-115/jailbreaker/chat.py", line 33, in main
    dialogs = json.load(f)    
  File "/hpc2n/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/json/__init__.py", line 293, in load
    return loads(fp.read(),
  File "/hpc2n/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
  File "/hpc2n/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/hpc2n/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Invalid control character at: line 5 column 168 (char 215)
[2023-10-22 13:48:47,885] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 988901) of binary: /pfs/proj/nobackup/fs/projnb10/hpc2n2023-115/env/bin/python
Traceback (most recent call last):
  File "/pfs/proj/nobackup/fs/projnb10/hpc2n2023-115/env/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/pfs/proj/nobackup/fs/projnb10/hpc2n2023-115/env/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/pfs/proj/nobackup/fs/projnb10/hpc2n2023-115/env/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/pfs/proj/nobackup/fs/projnb10/hpc2n2023-115/env/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/pfs/proj/nobackup/fs/projnb10/hpc2n2023-115/env/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/pfs/proj/nobackup/fs/projnb10/hpc2n2023-115/env/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/proj/nobackup/hpc2n2023-115/jailbreaker/chat.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-10-22_13:48:47
  host      : b-cn1602.hpc2n.umu.se
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 988901)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: b-cn1602: task 0: Exited with exit code 1
